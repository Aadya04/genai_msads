{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33b2950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import asyncio\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = os.getenv('HF_TOKEN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "132cb014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging configured\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    ")\n",
    "\n",
    "logging.getLogger(\"uvicorn.access\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "print(\"Logging configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b0bfa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "import a2a_patch\n",
    "import database_utility\n",
    "import service_tools\n",
    "import agent_definitions\n",
    "\n",
    "from server_launcher import start_server_daemon\n",
    "from client_runner import execute_test_suite\n",
    "\n",
    "print(\"All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ebf5874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING MULTI-AGENT SYSTEM\n",
      "================================================================================\n",
      "\n",
      "Waiting for all services to initialize...\n",
      "\n",
      "============================================================\n",
      "Initiating All Microservices...\n",
      "============================================================\n",
      "Initializing MCP Data Server...\n",
      "Database schema initialized and seeded at service_db.sqlite\n",
      "\n",
      "[MCP READY] Data Access Server listening on http://127.0.0.1:8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [74846]\n",
      "INFO:     Waiting for application startup.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Customer Info Agent starting on http://127.0.0.1:9300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [74846]\n",
      "INFO:     Waiting for application startup.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Support Specialist Agent starting on http://127.0.0.1:9301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [74846]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
      "INFO:     Uvicorn running on http://127.0.0.1:9300 (Press CTRL+C to quit)\n",
      "INFO:     Uvicorn running on http://127.0.0.1:9301 (Press CTRL+C to quit)\n",
      "INFO:     Uvicorn running on http://127.0.0.1:9400 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Orchestration Agent starting on http://127.0.0.1:9400\n",
      "\n",
      "[READY] All service servers deployed!\n",
      "    - Orchestration Entry Point: http://127.0.0.1:9400\n",
      "============================================================\n",
      "\n",
      "\n",
      "All servers running!\n",
      "\n",
      "Service Endpoints:\n",
      "   MCP Server:              http://127.0.0.1:8000\n",
      "   Customer Info Agent:     http://127.0.0.1:9300\n",
      "   Support Specialist:      http://127.0.0.1:9301\n",
      "   Orchestration Agent:     http://127.0.0.1:9400\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING MULTI-AGENT SYSTEM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "server_thread = start_server_daemon()\n",
    "\n",
    "print(\"\\nWaiting for all services to initialize...\")\n",
    "time.sleep(8)\n",
    "\n",
    "print(\"\\nAll servers running!\")\n",
    "print(\"\\nService Endpoints:\")\n",
    "print(\"   MCP Server:              http://127.0.0.1:8000\")\n",
    "print(\"   Customer Info Agent:     http://127.0.0.1:9300\")\n",
    "print(\"   Support Specialist:      http://127.0.0.1:9301\")\n",
    "print(\"   Orchestration Agent:     http://127.0.0.1:9400\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c144bf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RUNNING INTEGRATION TEST SUITE\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "INTEGRATION TEST SUITE - Multi-Agent Service System\n",
      "================================================================================\n",
      "\n",
      "[1/5] CASE 1: Simple Data Retrieval\n",
      "Notes: Routes to Information Agent for a basic lookup.\n",
      "Expected: Should return full customer record with all fields\n",
      "--------------------------------------------------------------------------------\n",
      "INFO:     127.0.0.1:59691 - \"GET /.well-known/agent-card.json HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59693 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 18:14:21,167 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: 3dc28ce0-fae8-4a33-9566-4a1ca98a4628, context_id: 912831b2-74f9-49cf-aafa-44215944415e).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59695 - \"GET /.well-known/agent-card.json HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 18:14:21,179 - INFO - a2a.client.card_resolver - Successfully fetched agent card data from http://localhost:9300/.well-known/agent-card.json: {'capabilities': {'streaming': True}, 'defaultInputModes': ['text/plain'], 'defaultOutputModes': ['text/plain', 'application/json'], 'description': 'Specialized system for secure access and management of customer records and data via a service layer.', 'name': 'Customer Information System', 'preferredTransport': 'JSONRPC', 'protocolVersion': '0.3.0', 'skills': [{'description': 'Fetches account details using the unique customer identifier.', 'examples': ['Find the record for ID 1', 'Retrieve customer 5 information'], 'id': 'get_details', 'name': 'Retrieve Customer Details', 'tags': ['customer', 'data', 'lookup']}, {'description': 'Amends customer fields such as email or phone.', 'examples': ['Update email for account 1', 'Change phone number for customer 5'], 'id': 'update_record', 'name': 'Modify Customer Record', 'tags': ['customer', 'update', 'modify']}, {'description': 'Execute complex queries requiring multiple database operations and filtering.', 'examples': ['Find all active accounts with open tickets', 'List customers with high priority issues'], 'id': 'complex_queries', 'name': 'Multi-Step Data Operations', 'tags': ['customer', 'search', 'filter', 'analysis']}], 'url': 'http://localhost:9300', 'version': '1.0'}\n",
      "2025-12-03 18:14:21,180 - INFO - google_adk.google.adk.agents.remote_a2a_agent - Successfully resolved remote A2A agent: information_system\n",
      "2025-12-03 18:14:21,218 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: 1c3ab391-14fb-402c-9090-22c795a502b9, context_id: d1f37770-498d-4b64-b2d3-826940ec4139).\n",
      "\u001b[92m18:14:21 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 18:14:21,222 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59700 - \"POST /call HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:14:22 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 18:14:22,799 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59695 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59706 - \"GET /.well-known/agent-card.json HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 18:14:23,816 - INFO - a2a.client.card_resolver - Successfully fetched agent card data from http://localhost:9301/.well-known/agent-card.json: {'capabilities': {'streaming': True}, 'defaultInputModes': ['text/plain'], 'defaultOutputModes': ['text/plain'], 'description': 'Dedicated agent for handling service inquiries, issue logging, and resolution.', 'name': 'Support Specialist', 'preferredTransport': 'JSONRPC', 'protocolVersion': '0.3.0', 'skills': [{'description': 'Logs a new ticket with customer ID, issue description, and priority level.', 'examples': ['Log a ticket for customer 1 about account upgrade', 'Create high priority billing issue'], 'id': 'log_issue', 'name': 'Register New Support Ticket', 'tags': ['support', 'ticket', 'create']}, {'description': 'Processes standard support questions and delivers a resolution or advice.', 'examples': ['I need help with my account', 'How do I upgrade my subscription?'], 'id': 'resolve_query', 'name': 'Address Customer Inquiry', 'tags': ['support', 'help', 'assistance']}], 'url': 'http://localhost:9301', 'version': '1.0'}\n",
      "2025-12-03 18:14:23,817 - INFO - google_adk.google.adk.agents.remote_a2a_agent - Successfully resolved remote A2A agent: specialist_support\n",
      "\u001b[92m18:14:23 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 18:14:23,829 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: f3532561-3d28-4be7-829e-19ca1fd7866e, context_id: 766358a8-35f4-418a-a3f4-75521eb70510).\n",
      "2025-12-03 18:14:23,828 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59707 - \"POST /call HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:14:24 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 18:14:24,806 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59706 - \"POST / HTTP/1.1\" 200 OK\n",
      "\n",
      "✓ RESPONSE:\n",
      "I have fetched the full record for customer ID 1. The customer's details are as follows:\n",
      "\n",
      "- Name: Alice Premium\n",
      "- Email: alice@example.com\n",
      "- Phone: 111-111-1111\n",
      "- Account Status: Active\n",
      "- Creation Timestamp: March 12, 2025\n",
      "- Last Modified Timestamp: March 12, 2025\n",
      "\n",
      "Now, I will proceed with creating a support ticket for the customer. Please provide the query description.\n",
      "\n",
      "[2/5] CASE 2: Coordinated Service Request\n",
      "Notes: Router should delegate to Support Agent to create an upgrade ticket.\n",
      "Expected: Should create a support ticket for service upgrade request\n",
      "--------------------------------------------------------------------------------\n",
      "INFO:     127.0.0.1:59712 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 18:14:31,081 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: c906bd83-e0c4-412c-aa95-345cf5230334, context_id: 27a031f4-77f4-4a59-9eb2-a7d4081acc04).\n",
      "\u001b[92m18:14:31 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 18:14:31,100 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: f8f8a27e-1035-4749-965f-bce97fd42aa7, context_id: 04a8d48a-339c-43fc-84ae-d3e1f993817d).\n",
      "2025-12-03 18:14:31,099 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "\u001b[92m18:14:31 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 18:14:31,819 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59714 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:14:33 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 18:14:33,146 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: 634e911c-9784-4e79-966c-d40d48545e6a, context_id: 15f80ddf-df8a-405e-98f3-0a4a66db2788).\n",
      "2025-12-03 18:14:33,145 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59719 - \"POST /call HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:14:34 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 18:14:34,186 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59718 - \"POST / HTTP/1.1\" 200 OK\n",
      "\n",
      "✓ RESPONSE:\n",
      "I think there might be a technical issue. Let me try again.\n",
      "\n",
      "To upgrade my service plan to Premium, I would like to change the plan type from the current one to the Premium plan. Additionally, I would like to request any other necessary changes to ensure a smooth transition to the new plan. Please confirm the ticket creation with the ticket ID.\n",
      "\n",
      "[3/5] CASE 3: Complex Filtering Query\n",
      "Notes: Requires multiple tool calls: get active accounts, check each for tickets, filter.\n",
      "Expected: Should list active accounts with at least one open ticket\n",
      "--------------------------------------------------------------------------------\n",
      "INFO:     127.0.0.1:59723 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 18:14:40,363 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: 8f2aaa1d-49e7-474c-9bbf-86703d61303e, context_id: e2204c49-ae29-4a3a-be22-e764f378c33e).\n",
      "\u001b[92m18:14:40 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 18:14:40,392 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: 8d7f1847-6876-4c02-b78b-0fa2ebb98379, context_id: 6de8cb44-0f79-4ff6-aa01-58ee84294fda).\n",
      "2025-12-03 18:14:40,392 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59727 - \"POST /call HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:14:41 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 18:14:41,529 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59725 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:14:43 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 18:14:43,649 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: 0e8d197e-fab3-44aa-9552-bc4c4a5d39b1, context_id: 3b958f70-2d01-4fc2-a131-3d03ee894da1).\n",
      "2025-12-03 18:14:43,649 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59735 - \"POST /call HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59736 - \"POST /call HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59737 - \"POST /call HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:59738 - \"POST /call HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:14:45 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 18:14:45,147 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59730 - \"POST / HTTP/1.1\" 200 OK\n",
      "\n",
      "✓ RESPONSE:\n",
      "Here are the active customer accounts with open support tickets:\n",
      "\n",
      "1. Alice Premium (identifier: 1) - contact_email: alice@example.com, contact_phone: 111-111-1111\n",
      "2. Bob Standard (identifier: 2) - contact_email: bob@example.com, contact_phone: 222-222-2222\n",
      "3. Diana Premium (identifier: 4) - contact_email: diana@example.com, contact_phone: 444-444-4444\n",
      "4. Eve Standard (identifier: 5) - contact_email: eve@example.com, contact_phone: 555-555-5555\n",
      "5. Priya Patel (Premium) (identifier: 12345) - contact_email: priya@example.com, contact_phone: 555-0999\n",
      "\n",
      "[4/5] CASE 4: High-Priority Ticket Logging\n",
      "Notes: Support Agent must recognize urgency and log a 'high' priority ticket.\n",
      "Expected: Should create high priority ticket for billing/refund issue\n",
      "--------------------------------------------------------------------------------\n",
      "INFO:     127.0.0.1:59744 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 18:14:51,971 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: 53ded57f-7193-442b-9191-b1050d403d28, context_id: 0f4728c4-509d-4fe8-a056-54bfd0443246).\n",
      "\u001b[92m18:14:51 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 18:14:51,988 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: 19f88b0a-0df5-4a46-8de7-297e3523d680, context_id: 81c5d6a1-fabe-4957-8c44-fee73df113a3).\n",
      "2025-12-03 18:14:51,987 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59748 - \"POST /call HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:14:55 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 18:14:55,614 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59750 - \"POST /call HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:14:56 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 18:14:56,773 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59746 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:14:59 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 18:14:59,024 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: 850e73fb-cfdf-4032-a939-3e0bce54ef1f, context_id: 8ab4f0bf-38a1-47db-9dd0-54171fb9bd9d).\n",
      "2025-12-03 18:14:59,023 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59756 - \"POST /call HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:15:00 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 18:15:00,514 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59754 - \"POST / HTTP/1.1\" 200 OK\n",
      "\n",
      "✓ RESPONSE:\n",
      "I've created a support ticket for your issue. The ticket ID is #1234. I'll need to look into the account details to determine which account was charged twice. Can you please provide me with the account ID that was charged twice? I'll use the `fetch_customer_data` tool to retrieve the customer details.\n",
      "\n",
      "Also, I'll need to confirm that you'd like to proceed with the refund request. If so, I'll use the `register_support_issue` tool again to update the ticket with the account ID.\n",
      "\n",
      "[5/5] CASE 5: Multi-Step Record Update and History Check\n",
      "Notes: Sequential: Update email (Data Agent) then retrieve history (Data Agent).\n",
      "Expected: Should update email successfully and display ticket history\n",
      "--------------------------------------------------------------------------------\n",
      "INFO:     127.0.0.1:59765 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 18:15:07,937 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: 4e253be8-6022-47ba-b514-8161b78c5344, context_id: bec595ae-e140-465c-b52b-e1a317164407).\n",
      "\u001b[92m18:15:07 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 18:15:07,955 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: c1330e42-fd08-4f85-b63f-227e5504aec9, context_id: e74bde6c-cf6e-41be-a198-4f2d49a19660).\n",
      "2025-12-03 18:15:07,955 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59770 - \"POST /call HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:15:09 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 18:15:09,341 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 18:15:09,614 - ERROR - google_adk.google.adk.a2a.executor.a2a_agent_executor - Error handling A2A request: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 156, in _make_common_async_call\n",
      "    response = await async_httpx_client.post(\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/litellm_core_utils/logging_utils.py\", line 190, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 449, in post\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 405, in post\n",
      "    response.raise_for_status()\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/httpx/_models.py\", line 829, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '402 Payment Required' for url 'https://router.huggingface.co/together/v1/chat/completions'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/402\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/main.py\", line 603, in acompletion\n",
      "    response = await init_response\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 288, in async_completion\n",
      "    response = await self._make_common_async_call(\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 181, in _make_common_async_call\n",
      "    raise self._handle_error(e=e, provider_config=provider_config)\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 3595, in _handle_error\n",
      "    raise provider_config.get_error_class(\n",
      "litellm.llms.huggingface.common_utils.HuggingFaceError: {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/a2a/executor/a2a_agent_executor.py\", line 167, in execute\n",
      "    await self._handle_request(context, event_queue)\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/a2a/executor/a2a_agent_executor.py\", line 237, in _handle_request\n",
      "    async for adk_event in agen:\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/runners.py\", line 454, in run_async\n",
      "    async for event in agen:\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/runners.py\", line 442, in _run_with_trace\n",
      "    async for event in agen:\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/runners.py\", line 654, in _exec_with_plugin\n",
      "    async for event in agen:\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/runners.py\", line 431, in execute\n",
      "    async for event in agen:\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/agents/base_agent.py\", line 291, in run_async\n",
      "    async for event in agen:\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/agents/llm_agent.py\", line 460, in _run_async_impl\n",
      "    async for event in agen:\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 346, in run_async\n",
      "    async for event in agen:\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 423, in _run_one_step_async\n",
      "    async for llm_response in agen:\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 801, in _call_llm_async\n",
      "    async for event in agen:\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 785, in _call_llm_with_tracing\n",
      "    async for llm_response in agen:\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 1038, in _run_and_handle_error\n",
      "    raise model_error\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 1024, in _run_and_handle_error\n",
      "    async for response in agen:\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/models/lite_llm.py\", line 1285, in generate_content_async\n",
      "    response = await self.llm_client.acompletion(**completion_args)\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/models/lite_llm.py\", line 141, in acompletion\n",
      "    return await acompletion(\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/utils.py\", line 1643, in wrapper_async\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/utils.py\", line 1489, in wrapper_async\n",
      "    result = await original_function(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/main.py\", line 622, in acompletion\n",
      "    raise exception_type(\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2328, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1624, in exception_type\n",
      "    raise APIError(\n",
      "litellm.exceptions.APIError: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "INFO:     127.0.0.1:59767 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m18:15:09 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 18:15:09,655 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: 03d47bbf-82b3-42da-bbe6-48fda0c885ee, context_id: f930406f-f276-4b5e-a4c9-3fd4dcbe9714).\n",
      "2025-12-03 18:15:09,654 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 18:15:09,839 - ERROR - google_adk.google.adk.a2a.executor.a2a_agent_executor - Error handling A2A request: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 156, in _make_common_async_call\n",
      "    response = await async_httpx_client.post(\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/litellm_core_utils/logging_utils.py\", line 190, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 449, in post\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/llms/custom_httpx/http_handler.py\", line 405, in post\n",
      "    response.raise_for_status()\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/httpx/_models.py\", line 829, in raise_for_status\n",
      "    raise HTTPStatusError(message, request=request, response=self)\n",
      "httpx.HTTPStatusError: Client error '402 Payment Required' for url 'https://router.huggingface.co/together/v1/chat/completions'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/402\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/main.py\", line 603, in acompletion\n",
      "    response = await init_response\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 288, in async_completion\n",
      "    response = await self._make_common_async_call(\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 181, in _make_common_async_call\n",
      "    raise self._handle_error(e=e, provider_config=provider_config)\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/llms/custom_httpx/llm_http_handler.py\", line 3595, in _handle_error\n",
      "    raise provider_config.get_error_class(\n",
      "litellm.llms.huggingface.common_utils.HuggingFaceError: {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/a2a/executor/a2a_agent_executor.py\", line 167, in execute\n",
      "    await self._handle_request(context, event_queue)\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/a2a/executor/a2a_agent_executor.py\", line 237, in _handle_request\n",
      "    async for adk_event in agen:\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/runners.py\", line 454, in run_async\n",
      "    async for event in agen:\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/runners.py\", line 442, in _run_with_trace\n",
      "    async for event in agen:\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/runners.py\", line 654, in _exec_with_plugin\n",
      "    async for event in agen:\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/runners.py\", line 431, in execute\n",
      "    async for event in agen:\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/agents/base_agent.py\", line 291, in run_async\n",
      "    async for event in agen:\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/agents/llm_agent.py\", line 460, in _run_async_impl\n",
      "    async for event in agen:\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 346, in run_async\n",
      "    async for event in agen:\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 423, in _run_one_step_async\n",
      "    async for llm_response in agen:\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 801, in _call_llm_async\n",
      "    async for event in agen:\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 785, in _call_llm_with_tracing\n",
      "    async for llm_response in agen:\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 1038, in _run_and_handle_error\n",
      "    raise model_error\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/flows/llm_flows/base_llm_flow.py\", line 1024, in _run_and_handle_error\n",
      "    async for response in agen:\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/models/lite_llm.py\", line 1285, in generate_content_async\n",
      "    response = await self.llm_client.acompletion(**completion_args)\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/google/adk/models/lite_llm.py\", line 141, in acompletion\n",
      "    return await acompletion(\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/utils.py\", line 1643, in wrapper_async\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/utils.py\", line 1489, in wrapper_async\n",
      "    result = await original_function(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/main.py\", line 622, in acompletion\n",
      "    raise exception_type(\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 2328, in exception_type\n",
      "    raise e\n",
      "  File \"/opt/anaconda3/envs/ml/lib/python3.10/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py\", line 1624, in exception_type\n",
      "    raise APIError(\n",
      "litellm.exceptions.APIError: litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "INFO:     127.0.0.1:59773 - \"POST / HTTP/1.1\" 200 OK\n",
      "\n",
      "✓ RESPONSE:\n",
      "litellm.APIError: HuggingfaceException - {\"error\":\"You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\"}\n",
      "\n",
      "================================================================================\n",
      "TEST SUITE SUMMARY\n",
      "================================================================================\n",
      "✓ PASS - CASE 1: Simple Data Retrieval\n",
      "✓ PASS - CASE 2: Coordinated Service Request\n",
      "✓ PASS - CASE 3: Complex Filtering Query\n",
      "✓ PASS - CASE 4: High-Priority Ticket Logging\n",
      "✓ PASS - CASE 5: Multi-Step Record Update and History Check\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Test suite completed successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING INTEGRATION TEST SUITE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    await execute_test_suite()\n",
    "    print(\"\\nTest suite completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nTest suite encountered an error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b407fd",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "A key takeaway was that reliable coordination depends on strict MCP tooling and clear message-based A2A design. One major challenge was periodic Hugging Face API token exhaustion, which temporarily broke agent responses and made debugging appear inconsistent.  Another challenge was LLM hallucinations producing unapproved intent names, which was mitigated using constrained prompts. These improvements made the system more resilient, even when model outputs were imperfect or external API limits were reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad9ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
